{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "X_scaled = pd.read_csv('../data/processed/X_scaled.csv')\n",
    "y = pd.read_csv('../data/processed/y.csv')['outcome']\n",
    "\n",
    "print(f\"Loaded data shape: {X_scaled.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22072244",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and clean data\n",
    "print(f\"Missing values in y: {y.isna().sum()}\")\n",
    "print(f\"Outcome distribution:\\n{y.value_counts()}\")\n",
    "print(f\"Outcome unique values: {y.unique()}\")\n",
    "\n",
    "# Remove rows with missing outcomes from BOTH X and y\n",
    "mask = ~y.isna()\n",
    "X_scaled_clean = X_scaled[mask]\n",
    "y_clean = y[mask]\n",
    "\n",
    "print(f\"\\nCleaned data shapes:\")\n",
    "print(f\"X: {X_scaled_clean.shape}, y: {y_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc9c0c",
   "metadata": {},
   "source": [
    "# 2. Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f69a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean)\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Train outcome distribution:\\n{pd.Series(y_train).value_counts()}\")\n",
    "print(f\"Test outcome distribution:\\n{pd.Series(y_test).value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8dfef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42) \n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911bb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806143e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"\\nXGBoost Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y_train))\n",
    "print(f\"Number of classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "print(\"Training Neural Network...\")\n",
    "\n",
    "# two hidden layers with dropout to prevent overfitting\n",
    "nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_nn = (nn_model.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\nNeural Network Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_nn):.4f}\")\n",
    "print(classification_report(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdedbd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Logistic Regression': accuracy_score(y_test, y_pred_lr),\n",
    "    'Random Forest': accuracy_score(y_test, y_pred_rf),\n",
    "    'XGBoost': accuracy_score(y_test, y_pred_xgb),\n",
    "    'Neural Network': accuracy_score(y_test, y_pred_nn)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"-\"*50)\n",
    "for model, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model:20s}: {acc:.4f}\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f207e7",
   "metadata": {},
   "source": [
    "# 3. Risk Stratification\n",
    "\n",
    "Convert binary predictions into three-class risk categories based on predicted probabilities for better clinical interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095aeead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_risk_category(probabilities, low_threshold=0.3, high_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Convert predicted probabilities to risk categories.\n",
    "    \n",
    "    Args:\n",
    "        probabilities: Array of predicted probabilities for positive class\n",
    "        low_threshold: Threshold below which risk is 'Low' (default: 0.3)\n",
    "        high_threshold: Threshold above which risk is 'High' (default: 0.7)\n",
    "    \n",
    "    Returns:\n",
    "        Array of risk categories: 0 (Low), 1 (Medium), 2 (High)\n",
    "    \"\"\"\n",
    "    risk_categories = np.zeros(len(probabilities), dtype=int)\n",
    "    risk_categories[(probabilities >= low_threshold) & (probabilities < high_threshold)] = 1\n",
    "    risk_categories[probabilities >= high_threshold] = 2\n",
    "    \n",
    "    return risk_categories\n",
    "\n",
    "print(\"Extracting predicted probabilities...\\n\")\n",
    "\n",
    "prob_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "risk_lr = predict_risk_category(prob_lr)\n",
    "\n",
    "prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "risk_rf = predict_risk_category(prob_rf)\n",
    "\n",
    "prob_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "risk_xgb = predict_risk_category(prob_xgb)\n",
    "\n",
    "prob_nn = nn_model.predict(X_test, verbose=0).flatten()\n",
    "risk_nn = predict_risk_category(prob_nn)\n",
    "\n",
    "print(\"Risk stratification complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying risk distribution for each model\n",
    "risk_labels = ['Low Risk', 'Medium Risk', 'High Risk']\n",
    "\n",
    "models_risk = {\n",
    "    'Logistic Regression': risk_lr,\n",
    "    'Random Forest': risk_rf,\n",
    "    'XGBoost': risk_xgb,\n",
    "    'Neural Network': risk_nn\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"RISK CATEGORY DISTRIBUTION\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for model_name, risk_pred in models_risk.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    unique, counts = np.unique(risk_pred, return_counts=True)\n",
    "    for risk_level, count in zip(unique, counts):\n",
    "        percentage = (count / len(risk_pred)) * 100\n",
    "        print(f\"  {risk_labels[risk_level]:15s}: {count:4d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_array = np.array(y_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"RISK STRATIFICATION PERFORMANCE (XGBoost)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "risk_performance = pd.DataFrame({\n",
    "    'Risk Category': risk_labels,\n",
    "    'Count': [np.sum(risk_xgb == i) for i in range(3)],\n",
    "    'Actual Deaths': [np.sum(y_test_array[risk_xgb == i]) for i in range(3)],\n",
    "    'Death Rate (%)': [np.mean(y_test_array[risk_xgb == i]) * 100 if np.sum(risk_xgb == i) > 0 else 0 for i in range(3)]\n",
    "})\n",
    "\n",
    "print(risk_performance.to_string(index=False))\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Low Risk: Should have low death rate\")\n",
    "print(\"- Medium Risk: Intermediate death rate (uncertain cases)\")\n",
    "print(\"- High Risk: Should have high death rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Predicted Probability Distributions by Risk Category', fontsize=16, fontweight='bold')\n",
    "\n",
    "models_prob = [\n",
    "    ('Logistic Regression', prob_lr, risk_lr),\n",
    "    ('Random Forest', prob_rf, risk_rf),\n",
    "    ('XGBoost', prob_xgb, risk_xgb),\n",
    "    ('Neural Network', prob_nn, risk_nn)\n",
    "]\n",
    "\n",
    "for idx, (name, probs, risks) in enumerate(models_prob):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # plotting histogram for each risk category\n",
    "    colors = ['green', 'orange', 'red']\n",
    "    for risk_level in range(3):\n",
    "        mask = risks == risk_level\n",
    "        if np.sum(mask) > 0:\n",
    "            ax.hist(probs[mask], bins=20, alpha=0.6, label=risk_labels[risk_level], \n",
    "                   color=colors[risk_level], edgecolor='black')\n",
    "    \n",
    "    ax.axvline(x=0.3, color='blue', linestyle='--', linewidth=1.5, label='Low/Med threshold')\n",
    "    ax.axvline(x=0.7, color='purple', linestyle='--', linewidth=1.5, label='Med/High threshold')\n",
    "    \n",
    "    ax.set_xlabel('Predicted Probability', fontsize=11)\n",
    "    ax.set_ylabel('Frequency', fontsize=11)\n",
    "    ax.set_title(name, fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75426fc",
   "metadata": {},
   "source": [
    "# Evaluate 3-class risk prediction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdd265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"3-CLASS RISK STRATIFICATION PERFORMANCE\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "models_risk_eval = {\n",
    "    'Logistic Regression': risk_lr,\n",
    "    'Random Forest': risk_rf,\n",
    "    'XGBoost': risk_xgb,\n",
    "    'Neural Network': risk_nn\n",
    "}\n",
    "\n",
    "for model_name, risk_pred in models_risk_eval.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # create a simplified 2-class version for comparison (Low vs High)\n",
    "    # treat Medium as Low risk for evaluation purposes\n",
    "    risk_binary = risk_pred.copy()\n",
    "    risk_binary[risk_binary == 1] = 0  # Medium -> Low for comparison\n",
    "    risk_binary[risk_binary == 2] = 1  # High -> High\n",
    "    \n",
    "    print(classification_report(y_test_array, risk_binary, \n",
    "                                target_names=['Low/Medium Risk', 'High Risk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad963a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "# create binary version: Low/Medium vs High\n",
    "risk_binary_xgb = risk_xgb.copy()\n",
    "risk_binary_xgb[risk_binary_xgb == 1] = 0\n",
    "risk_binary_xgb[risk_binary_xgb == 2] = 1\n",
    "\n",
    "cm = confusion_matrix(y_test_array, risk_binary_xgb)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Predicted: Low/Med', 'Predicted: High'],\n",
    "            yticklabels=['Actual: Survived', 'Actual: Died'])\n",
    "\n",
    "ax.set_title('Risk Stratification Confusion Matrix (XGBoost)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Actual Outcome', fontsize=12)\n",
    "ax.set_xlabel('Predicted Risk Category', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8523567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "risk_results = {}\n",
    "\n",
    "for model_name, risk_pred in models_risk_eval.items():\n",
    "    # convert to binary for fair comparison\n",
    "    risk_binary = risk_pred.copy()\n",
    "    risk_binary[risk_binary == 1] = 0  # Medium -> Low\n",
    "    risk_binary[risk_binary == 2] = 1  # High\n",
    "    \n",
    "    acc = accuracy_score(y_test_array, risk_binary)\n",
    "    risk_results[model_name] = acc\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"RISK STRATIFICATION MODEL COMPARISON\")\n",
    "print(\"-\"*60)\n",
    "for model, acc in sorted(risk_results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model:20s}: {acc:.4f}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# compare with original binary predictions\n",
    "print(\"\\nComparison with Original Binary Predictions:\")\n",
    "print(\"-\" * 60)\n",
    "for model in results.keys():\n",
    "    print(f\"{model:20s} - Binary: {results[model]:.4f} | Risk: {risk_results[model]:.4f}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc98c76",
   "metadata": {},
   "source": [
    "### dataset is heavily imbalanced. will apply smote in the next file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
